{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fd1a9d3-8069-47cb-9800-70db20bc9d5e",
   "metadata": {},
   "source": [
    "In this notebook, we are going to fine-tune a pre-existing language model on a new dataset using a technique called Low-rank Adaptation (LoRA) on Intel discrete GPUs. We're going to use the Hugging Face Transformers library to handle the model and the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17e87e-5359-413a-af3a-fc47be60ff19",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's import the necessary libraries for our fine-tuning task. These include libraries for data manipulation (pandas), transformers (transformers), PyTorch (torch), and the Intel extension for PyTorch (intel_extension_for_pytorch), among others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d7e7fd-7eee-483d-b11a-2917ece41e68",
   "metadata": {},
   "source": [
    "### Import necessary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd262cc-19a9-4996-ae99-6f8ef35b8209",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devcloud/miniconda/envs/pytorch_xpu/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "import fire\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "from transformers import LlamaForCausalLM\n",
    "from transformers import LlamaTokenizer\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ca52d-5246-4802-af7c-3f8ab4e96212",
   "metadata": {},
   "source": [
    "Now let's set the environment for the Intel extension for PyTorch (IPEX). IPEX is used to accelerate deep learning inference and training by using Intel's hardware and software capabilities and also importantly to give torch the `xpu` namespace, so that you can use things like `torch.xpu.get_device_name()`.\n",
    "\n",
    "Here we're forcing IPEX to treat multiple slices (compute blocks) of the dGPU as a single GPU. This is essential for our use case as we're working with a specific GPU (e.g., 1550 dGPU) that has multiple slices per dGPU and also want to utilize the full 128 GB VRAM of the dGPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88931588-41c0-4df1-b29e-f874390e9a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"IPEX_TILE_AS_DEVICE\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c38a5d-0dfa-4591-a195-9b4492f88abe",
   "metadata": {},
   "source": [
    "Check to see if XPU (Intel's XPU is a mix of CPUs, GPUs, FPGAs, and AI accelerators) is available for training. If available, the device name of the XPU is printed. For today, we will be using a GPU - The Intel Data Centre Max GPU - 1550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87e37a90-cab0-4e1a-b073-f1d1860e33bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for XPU availability\n",
    "if torch.xpu.is_available():\n",
    "    print(\"Using '{}' as an xpu device.\".format(torch.xpu.get_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e0705e-7e5a-4a70-8446-02fc1f84e566",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we define the hyperparameters and configuration for our model and the LoRA process. These settings include the LoRA parameters (rank r, alpha value lora_alpha, target modules, and dropout rate), the batch sizes, the number of training steps, the learning rate, and the directory where we'll save our outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776dbebd-77a5-400b-8f10-e4e408a69c90",
   "metadata": {},
   "source": [
    "### LoRA - What is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b195ea0f-bfd2-415c-a9ca-0a504b237b80",
   "metadata": {
    "tags": []
   },
   "source": [
    "Before that, let's understand what LoRA is and why it is significant. \n",
    "\n",
    "Low-Rank Adaptation, or LoRA, is a method to fine-tune large language models (LLMs). LLMs are pre-trained on diverse, large-scale datasets to gain general language understanding. The problem, however, is that fine-tuning these models on specific tasks is computationally expensive and often requires substantial resources.\n",
    "\n",
    "LoRA is a technique that allows us to effectively fine-tune these models without being computationally expensive as traditional methods. The idea is to restrict the fine-tuning to a low-rank subspace of the original parameter space. Instead of updating all parameters of the model during fine-tuning, we update only a small fraction of them (the ones corresponding to this low-rank subspace), making the process more efficient.\n",
    "\n",
    "The key idea is to reduce the complexity of the fine-tuning process by focusing only on a small, carefully chosen subset of the model's parameters.\n",
    "\n",
    "To further understand this, you need to know what a 'rank' of a matrix is. In simple terms, the rank of a matrix in linear algebra is a measure of the 'dimensionality' of the information it contains. A lower rank means that the matrix can represent fewer dimensions, and consequently, carries less information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c7370d-38ab-4f8f-a5dc-6b0489bf98db",
   "metadata": {
    "tags": []
   },
   "source": [
    "![low rank decompose](\"./images/lora_decompose.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16fd29-b179-44dc-9e2f-f52a7d18b95a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "When we say a 'low-rank' matrix, we are essentially talking about a simpler, compressed representation of the original data. The 'low-rank' characteristic refers to the fact that this matrix has fewer dimensions, but these dimensions are chosen in such a way that they capture the most important aspects of the data.\n",
    "\n",
    "In the case of LoRA, the low-rank matrix is designed to capture the essential information required to adapt the pre-trained model to the new, specific task. The goal of introducing a low-rank matrix in LoRA is not to discard information, but to distill and concentrate the fine-tuning updates into this matrix. This allows us to get the most 'bang for our buck' â€” achieving effective fine-tuning while dramatically reducing the computational complexity of the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd1c185-dbc0-4290-9b14-95e767f4f075",
   "metadata": {},
   "source": [
    "To use LoRA, we create a LoraConfig object. This is a configuration class provided by the peft library, which stands for Pretraining with Effective Fine-Tuning. This class accepts various parameters that control the behavior of LoRA:\n",
    "\n",
    "    r: The 'rank' of the LoRA matrix. This controls the complexity of the LoRA matrix and hence, the amount of information it can represent. A lower rank will result in a simpler matrix and hence faster, less resource-intensive fine-tuning, while a higher rank will allow for more complex adaptations but at the cost of increased computational resources. Here we set it to 8, a value that strikes a good balance between efficiency and effectiveness.\n",
    "\n",
    "    lora_alpha: The 'alpha' hyperparameter in LoRA. This controls the strength of the LoRA adaptation relative to the original model parameters. Higher values will result in the LoRA matrix having a stronger influence on the fine-tuning updates, while lower values will result in the original model parameters having a stronger influence. The choice of this hyperparameter depends on the specific task and dataset, and may require some experimentation to optimize. According to the original paper introducing LoRA, the learning rate for the low-rank matrix should be higher than that of the bias terms, so this parameter is used to scale up the learning rate.\n",
    "\n",
    "    target_modules: The layers or modules of the model to which LoRA is applied. Here we specify that LoRA is applied to the \"q_proj\" and \"k_proj\" layers of the Transformer model, which are part of the attention mechanism.\n",
    "\n",
    "    lora_dropout: The dropout rate for the LoRA layers. Dropout is a regularization technique that randomly 'drops out' (i.e., sets to zero) a proportion of the layer's outputs during training, to prevent overfitting. Here we set it to 0.05, indicating a 5% dropout rate.\n",
    "\n",
    "    bias: The type of bias to use in the LoRA layers. Here we set it to \"none\", indicating no bias is used.\n",
    "\n",
    "    task_type: The type of task for which the model is being fine-tuned. Here we set it to \"CAUSAL_LM\", indicating that we're fine-tuning for a causal language modeling task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f34473d9-eddd-489e-850c-39cf26d1aa2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters and configuration\n",
    "# Low-rank adaptation (LoRA) parameters\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df94265-04a3-438a-ba84-63b05700e947",
   "metadata": {},
   "source": [
    "LoRA enables us to 'distill' the essence of the fine-tuning updates into this low-rank matrix, reducing the computational complexity of the fine-tuning process. This makes the process more efficient, requiring less memory and compute resources, and enables the fine-tuning of LLMs even on devices with modest hardware specifications.\n",
    "\n",
    "Furthermore, by focusing the learning on a low-rank matrix, LoRA also helps to prevent 'catastrophic forgetting', a common problem in fine-tuning where the model forgets the knowledge it gained during pre-training. Since the original parameters of the model remain largely unchanged in LoRA, the model retains its pre-training knowledge, which often results in better performance on the downstream tasks.\n",
    "\n",
    "The LoRA configuration is then passed to the get_peft_model() function, which modifies the original Transformer model to include the LoRA adaptation. The fine-tuned model can then be trained as usual. Thus, the real magic of LoRA is its ability to find a balance between efficiency (using a low-rank matrix) and effectiveness (capturing the most critical information for fine-tuning). This makes it a highly practical and powerful approach for fine-tuning large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a6f4c4-a157-4bac-ae23-4431f33c4670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Other config and hyper params\n",
    "# Training parameters\n",
    "BATCH_SIZE = 128\n",
    "MICRO_BATCH_SIZE = 12\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 3e-4\n",
    "TRAIN_STEPS = 100\n",
    "OUTPUT_DIR = \"experiments\"\n",
    "CUTOFF_LEN = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc617bca-debb-46c6-a7d6-8879cce957d4",
   "metadata": {},
   "source": [
    "###  Base Model and tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80f575-cfb9-4bf5-ad3c-d877a293b229",
   "metadata": {},
   "source": [
    "Now, let's define the base language model that we will use for fine-tuning. The base model is specified by its identifier, which is a string that points to a particular model in the Hugging Face model hub. In this case, the base model is \"openlm-research/open_llama_3b\".\n",
    "\n",
    "Then, we load the base model and its associated tokenizer using the from_pretrained method. This method fetches the model and tokenizer from the Hugging Face model hub.\n",
    "\n",
    "Finally, we set the padding configuration for the tokenizer. Padding is used to ensure that all sequences in a batch have the same length. The pad token id is set to 0, and the padding is done to the left of the sequences. This is because causal language models like Llama are trained to predict the next token in a sequence, so they only need to look at the previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b2ecbf-d6b2-4a1a-aadf-6671149d0d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define base model\n",
    "BASE_MODEL = \"openlm-research/open_llama_3b\"\n",
    "\n",
    "# Load the pretrained base model and tokenizer\n",
    "model = LlamaForCausalLM.from_pretrained(BASE_MODEL)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Set padding configuration for tokenizer\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77589b7a-bc23-4c15-b63e-4fdcfc0c3276",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90492117-6136-4a80-acb6-5786c9477e50",
   "metadata": {},
   "source": [
    "Several helper functions are defined to facilitate the data preprocessing for model training.\n",
    "\n",
    "    print_trainable_parameters(model): This function takes a PyTorch model as an input and prints the number of trainable parameters in the model. It iterates through all parameters of the model and counts the ones that require gradient computation (i.e., the ones that will be updated during training).\n",
    "\n",
    "    prompter(data): This function takes a sample from the Simpsons dataset and formats it into a dialogue prompt. The prompt consists of an instruction, input, and a response.\n",
    "\n",
    "    tokenize(prompt, add_eos_token=True): This function takes a prompt, tokenizes it using the model's tokenizer, and optionally adds an End of Sentence (EOS) token at the end. The tokenized prompt is then returned.\n",
    "\n",
    "    generate_and_tokenize_prompt(data): This function combines the above steps by generating a dialogue prompt from the given data and then tokenizing this prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd183ee3-3c66-4f08-bade-c2659f5fdfff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    # ... code ...\n",
    "\n",
    "def prompter(data):\n",
    "    \"\"\"\n",
    "    Format a dialogue prompt from Simpsons dataset.\n",
    "    \"\"\"\n",
    "    # ... code ...\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    \"\"\"\n",
    "    Tokenize the prompt and add EOS token if not present.\n",
    "    \"\"\"\n",
    "    # ... code ...\n",
    "\n",
    "def generate_and_tokenize_prompt(data):\n",
    "    \"\"\"\n",
    "    Generate a dialogue prompt and tokenize it.\n",
    "    \"\"\"\n",
    "    # ... code ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ae11e-4949-4ed5-a348-ff214db5363a",
   "metadata": {},
   "source": [
    "### LoRa Configuration & Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eaf931-776d-41b0-ab1d-0fd2cc64dad3",
   "metadata": {},
   "source": [
    "Now finally we are in the `main` section of the code. Here, first, a LoraConfig object is created to configure the LoRa parameters for the model. Then, the base model is wrapped with the LoRa model using get_peft_model() function. This function prepares our model for fine-tuning with LoRA by adding the low-rank matrices to the specified layers of the model.\n",
    "\n",
    "Finally, we print the number of trainable parameters in our model by calling the print_trainable_parameters function. This gives us an indication of the complexity of our model and the computational resources required to fine-tune it.\n",
    "\n",
    "Next, it checks if the preprocessed dataset is available in the disk. If it is, the dataset is loaded; otherwise, the raw data is loaded and preprocessed. The train_val object is a split of the dataset into training and validation sets, while train_data and val_data are tokenized versions of the corresponding datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c302ecb4-5d26-433a-b6f1-3feee850b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configure LoRA for the model\n",
    "    config = LoraConfig(\n",
    "        # ... parameters ...\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "    print(print_trainable_parameters(model))\n",
    "\n",
    "    # Load data\n",
    "    # ... loading and preprocessing code ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955cb877-f608-4543-a6c7-c6e1dba27d68",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Prepration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08de9f7-562b-4e54-b6d2-6479d81ca7a3",
   "metadata": {},
   "source": [
    "For training, we use the TrainingArguments class from the Hugging Face transformers library to define the training configurations.\n",
    "\n",
    "Then, we print out some details of the training configuration, such as the process rank, the device used for training, the number of GPUs, and the type of distributed training.\n",
    "\n",
    "Following that, a data collator is created, which is used to collate individual data samples into a batch for training. The DataCollatorForSeq2Seq class is used, which is specifically designed for sequence-to-sequence models.\n",
    "\n",
    "Finally, a Trainer object is initialized with the model, training dataset, evaluation dataset, training arguments, and data collator. This object is used to manage the training and evaluation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c155fdc-402f-40c4-afd3-fa5cad7fa698",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Important HuggingFace Trainer Training Arguments for Intel dGPUs (XPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7656b5-cff7-4d3b-9845-fffdfc0afc58",
   "metadata": {},
   "source": [
    "\n",
    "- **bf16**: This argument is used to specify whether to use BFloat16 precision for model training or not. BFloat16 can offer better training speed and memory utilization while providing acceptable levels of model accuracy.\n",
    "- **no_cuda**: This argument is set to True to indicate we are not using 'cuda' to train the model\n",
    "- **use_xpu**: This argument is set to True to indicate that the model training should be performed on the Intel GPU (XPU). Note that for this option to work, the necessary Intel PyTorch extensions and drivers need to be properly installed and the Intel GPU needs to be available in the system.\n",
    "- **use_ipex**: This argument is set to True to indicate that the Intel PyTorch Extension (IPEX) should be used. IPEX can provide better model performance on Intel hardware by utilizing low precision computation and other optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc7e9b-1f60-48a8-8540-be1f2d1d7fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_arguments = transformers.TrainingArguments(\n",
    "    # ... parameters ...\n",
    ")\n",
    "\n",
    "# Printing training configuration\n",
    "# ... print statements ...\n",
    "\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    # ... parameters ...\n",
    ")\n",
    "\n",
    "# Huggingface Trainer config\n",
    "trainer = transformers.Trainer(\n",
    "    # ... parameters ...\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f2d54-0a09-43e9-8829-e2e0d3146da7",
   "metadata": {},
   "source": [
    "###  Training & Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce1af9-cbcc-4eb9-988a-127f8b18569e",
   "metadata": {},
   "source": [
    "Before training, we update the state_dict() function of the model. This function returns the model's parameters, and it's used by PyTorch to save and load models. The update is required because we have wrapped the base model with LoRa, so we need to make sure that the state_dict() function returns the correct parameters.\n",
    "\n",
    "When we save a model in PyTorch, it uses a method called state_dict() to gather all the parameters and their respective states. This state_dict is a Python dictionary object that maps each layer to its parameter tensor.\n",
    "\n",
    "For a standard PyTorch model, state_dict() works perfectly fine, but when we modify the model's architecture, as we do here with LoRa (Low-Rank Adaptation), we are essentially creating additional parameters (the low-rank matrices) that don't exist in the original model's structure. The base model's state_dict() doesn't know about these additional parameters and hence, won't include them when called.\n",
    "\n",
    "By replacing the state_dict() method with get_peft_model_state_dict(), we're ensuring that the LoRa parameters are included when the model's state is saved or loaded. In other words, this method returns the state dictionary of the model, including the extra parameters added by LoRa.\n",
    "\n",
    "This way, when we save the model after training and load it back later for inference, the model's state also includes these extra parameters, allowing it to perform as expected. Without this modification, the saved model wouldn't work correctly because it would be missing the LoRa parameters.\n",
    "\n",
    "Then, we call trainer.train() to start the training process. Once training is completed, the model is saved to the specified output directory using the save_pretrained() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fecc445-e4bc-40a8-aabf-5d12044338f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n",
    ").__get__(model, type(model))\n",
    "\n",
    "# train and save the model\n",
    "trainer.train()\n",
    "model.save_pretrained(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3f8e6-42ee-4ab0-9d11-87eafa22ee9f",
   "metadata": {},
   "source": [
    "In conclusion, in this code we fine-tune a transformer language model using the Hugging Face transformers library with a technique called Low-Rank Adaptation (LoRa). This method allows us to adapt a large pre-trained language model for a specific task with fewer trainable parameters. We use the Simpsons dialogues dataset to fine-tune the language model to generate plausible responses to dialogue prompts. It is structured to run efficiently on an Intel GPU, utilizing the PyTorch extension for Intel devices, and it employs the Trainer API from Hugging Face for training and evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_xpu",
   "language": "python",
   "name": "pytorch_xpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
